# 10. AWS S3 (Simple Storage Service) 이해와 활용

자산운용 플랫폼 아키텍처에서 **AWS S3**가 왜 핵심적인 역할을 하는지, 그리고 왜 대용량 금융 데이터를 저장하는 데 최적인지에 대한 가이드입니다.

---

## ☁️ S3란 무엇인가? (Simple Storage Service)

AWS S3는 인터넷 어디서나 원하는 양의 데이터를 저장하고 검색할 수 있도록 구축된 **객체 스토리지(Object Storage)** 서비스입니다. 일반적인 하드디스크(파일 시스템)와 달리, 데이터를 '객체'라는 단위로 관리하며 고유한 ID(키)를 통해 접근합니다.

### 핵심 특징 (왜 쓰는가?)
1. **무한한 확장성 (Scalability)**: 데이터 용량에 제한이 없습니다. 몇 기가바이트(GB)에서 몇 페타바이트(PB)까지 자동으로 확장됩니다.
2. **압도적인 내구성 (Durability)**: 99.999999999%(11개)의 내구성을 보장하도록 설계되어, 데이터 유실 걱정이 거의 없습니다.
3. **비용 효율성 (Cost-effective)**: 사용한 만큼만 비용을 지불하며, 빈번하지 않은 데이터는 더 저렴한 티어(Glacier 등)로 옮겨 비용을 절감할 수 있습니다.
4. **보안 및 권한 관리**: IAM(Identity and Access Management)을 통해 파일 하나하나에 대한 접근 권한을 정교하게 제어할 수 있습니다.

---

## 🏗 자산운용 플랫폼에서 S3를 쓰는 이유

### 1. '데이터 레이크(Data Lake)'의 기반
자산운용사는 시장 데이터, 재무 데이터, 뉴스, 소셜 미디어 데이터 등 방대한 양의 데이터를 수집합니다. 이를 DB에 다 넣기에는 비용과 성능 부담이 큽니다. S3는 이 모든 로우 데이터를 담아두는 거대한 **데이터 호수** 역할을 합니다.

### 2. 컴퓨팅과 저장소의 분리 (Decoupling)
쿠버네티스(K8s) 클러스터에서 백테스팅 엔진 Pod가 늘어나거나 줄어들 때, 데이터가 특정 서버의 디스크에 있으면 공유하기 어렵습니다. S3에 데이터를 두면 수백 개의 Pod가 동시에 동일한 Parquet 파일에 접근하여 병렬 연산을 수행할 수 있습니다.

### 3. 분석 도구와의 뛰어난 호환성
- **Spark / Presto / Athena**: S3에 저장된 Parquet 파일을 DB에 적재하지 않고도 SQL로 직접 쿼리할 수 있습니다.
- **Python (Pandas/PyArrow)**: `pd.read_parquet('s3://...')` 한 줄로 클라우드상의 대용량 데이터를 메모리로 즉시 읽어올 수 있습니다.

---

## ⚔️ S3 vs 데이터베이스(DB) vs EBS(서버 디스크)

| 구분 | AWS S3 (객체 스토리지) | PostgreSQL (DB) | EBS (블록 스토리지) |
| :--- | :--- | :--- | :--- |
| **데이터 형태** | 파일, 이미지, 대량 로그 | 정형 데이터, 관계 | 서버 실행 파일, DB 저장소 |
| **주요 장점** | 저렴한 비용, 무한 확장 | 빠른 검색, 트랜잭션 | 가장 빠른 응답 속도 |
| **금융 활용** | **백테스트용 시계열 데이터** | **종목 마스터, 매매 로그** | **서버 운영체제(OS)** |

---

## 🚀 백엔드 엔지니어의 핵심 멘트 (면접 답변용)

> "S3는 단순히 파일을 저장하는 공간을 넘어, 현대적인 데이터 아키텍처의 핵심인 **'컴퓨팅과 저장소의 분리'**를 가능케 하는 도구입니다. 
> 
> 특히 미래에셋자산운용처럼 글로벌 지사에서 대규모 퀀트 연산을 수행해야 하는 경우, 데이터를 중앙 집중형 S3에 보관하고 각 지사의 쿠버네티스 노드들이 필요한 만큼만 끌어다 쓰는 구조가 가장 효율적입니다. 
> 
> 저는 21억 건의 데이터를 다루며 DB 성능 한계를 경험했을 때, **정적 데이터는 S3 + Parquet 구조로 이관**하여 쿼리 비용을 80% 이상 절감하고 조회 속도를 대폭 개선한 바 있습니다."

---

## 💻 실무 활용 예시 (Python)

```python
import pandas as pd
import s3fs

# S3 버킷에 저장된 10년치 삼성전자 주가 Parquet 읽기
# 네트워크 오버헤드를 최소화하기 위해 'snappy' 압축 방식 주로 사용
s3_path = 's3://mirae-asset-quant-data/refined/price/005930.parquet'
df = pd.read_parquet(s3_path)

print(f"로드 완료: {len(df)} 행")
```

이 문서는 단순한 기술 설명을 넘어, 자산운용사의 대규모 인프라를 효율적으로 운영하기 위한 **비용/성능 최적화 전략**을 이해하고 있음을 보여줍니다.
